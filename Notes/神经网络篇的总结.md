---
title: 神经网络篇的总结
date: 2020-07-04 09:53
---

题外话：学习神经网络的时候，因为实习各种事儿耽搁了不少时间。断断续续的学，学了后面忘了前面，效果不是很好，所以在这里重温一遍吴恩达循序渐进的教学过程。<https://zhuanlan.zhihu.com/c_128528277>

非线性的分类问题 -> 构造一个有很多项的非线性的逻辑回归函数
如果特征量 n 特别多（尤其对于图片来说） -> 只考虑二阶项 -> n^2/2 -> 计算量大、过拟合
-> 引入神经网络


有s型函数（逻辑函数）作为激励的人工神经元：sigmoid (logistic) activation function 

**熟练掌握**：神经网络每一层变量之间的关系（Forward Propagation），并会用向量表示。


神经网络 VS 逻辑回归：输入不是直接应用 h 函数（简单逻辑回归的做法），而是添加了另一层 a 来学习特征间的关系。

举例：AND 、 OR 、NOT 等函数

多分类问题的输出表示
- - - - - 
回顾了一下上节课的内容，单分类 和 多分类 的输出表示

对照逻辑回归的 cost function，提出神经网络对应的 cost function

如何计算梯度？~~梯度下降算法~~❌ 反向传播算法 ✅ 
PS：我在 notability 中推导了 error 计算过程[red]

**熟练掌握**：反向传播算法的原理、公式推导、算法实现

因为梯度计算比较复杂，如何保证 bug-free 呢？**gradApprox** 有个公式，最后需要放在算法实现验证，真正训练时注释掉。

那神经网络的参数怎么挑选呢？**随机化**。




